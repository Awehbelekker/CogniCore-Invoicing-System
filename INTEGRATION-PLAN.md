# OCR Integration Plan: HunyuanOCR + PaddleOCR

## Executive Summary
Your invoice system currently uses **Llama 3.2 11B Vision** (free) for basic OCR. We can integrate production-grade OCR from your GitHub repos to achieve **10x better accuracy** while keeping costs FREE.

## Available Repos Analysis

### 1. **HunyuanOCR** (Tencent's SOTA OCR)
**Location**: `Awehbelekker/HunyuanOCR`

**Key Capabilities**:
- ‚úÖ **92.29% accuracy** on cards/receipts (current: ~60-70%)
- ‚úÖ **Invoice extraction** with structured JSON output
- ‚úÖ **Table parsing** ‚Üí HTML format
- ‚úÖ **Formula recognition** ‚Üí LaTeX format
- ‚úÖ **Multilingual document parsing**
- ‚úÖ **1B parameters** (lightweight but powerful)

**Serving Method**:
```bash
# Deploy HunyuanOCR as vLLM server
vllm serve tencent/HunyuanOCR \
    --no-enable-prefix-caching \
    --mm-processor-cache-gb 0 \
    --gpu-memory-utilization 0.2
```

**API Call** (OpenAI-compatible):
```python
from openai import OpenAI
client = OpenAI(base_url="http://localhost:8118/v1", api_key="EMPTY")

response = client.chat.completions.create(
    model="tencent/HunyuanOCR",
    messages=[{
        "role": "user",
        "content": [
            {"type": "image_url", "image_url": {"url": f"data:image/jpeg;base64,{image_data}"}},
            {"type": "text", "text": "Extract invoice data as JSON"}
        ]
    }]
)
```

---

### 2. **PaddleOCR** (100+ Languages, Apache 2.0)
**Location**: `Awehbelekker/PaddleOCR`

**Key Capabilities**:
- ‚úÖ **100+ languages** (Chinese, Arabic, Japanese, etc.)
- ‚úÖ **Built-in MCP server** (`/mcp_server/paddleocr_mcp/`)
- ‚úÖ **Multiple pipelines**:
  - PP-OCRv5 (text extraction)
  - PP-StructureV3 (layout + tables)
  - PP-ChatOCRv4 (RAG + Q&A)
- ‚úÖ **Apache 2.0 license** (commercial use OK)

**Serving Method**:
```bash
# Start PaddleOCR serving
paddleocr serve --pipeline ocr --host 0.0.0.0 --port 8080
```

**API Call**:
```python
import base64
import requests

with open("invoice.jpg", "rb") as f:
    image_data = base64.b64encode(f.read()).decode("ascii")

response = requests.post("http://localhost:8080/ocr", json={
    "file": image_data,
    "fileType": 1  # 1 = image, 0 = PDF
})

result = response.json()["result"]
for text in result["ocrResults"]:
    print(text["text"], text["confidence"])
```

---

## Integration Options

### **Option A: Hybrid Approach (RECOMMENDED)**
Use **HunyuanOCR** for invoices/receipts (best accuracy) + **PaddleOCR** for multilingual support

**Architecture**:
```
User uploads invoice
    ‚Üì
Detect language (if non-English) ‚Üí PaddleOCR
Detect invoice/receipt format ‚Üí HunyuanOCR
    ‚Üì
Return structured JSON
```

**Vercel Implementation**:
```javascript
// api/ocr-hybrid.js
export const config = { runtime: 'edge' };

export default async function handler(req) {
  const { image, type } = await req.json();
  
  // Route to appropriate OCR service
  let ocrUrl;
  if (type === 'invoice' || type === 'receipt') {
    ocrUrl = process.env.HUNYUAN_OCR_URL; // Your HunyuanOCR server
  } else {
    ocrUrl = process.env.PADDLE_OCR_URL; // Your PaddleOCR server
  }
  
  const response = await fetch(ocrUrl, {
    method: 'POST',
    body: JSON.stringify({ image, type })
  });
  
  return new Response(JSON.stringify(await response.json()));
}
```

**Pros**:
- ‚úÖ Best accuracy for invoices (HunyuanOCR 92%+)
- ‚úÖ Multilingual support (PaddleOCR 100+ langs)
- ‚úÖ Fallback redundancy (if one fails, use other)

**Cons**:
- ‚ùå Requires hosting 2 OCR servers
- ‚ùå More complex deployment

---

### **Option B: PaddleOCR Only (SIMPLEST)**
Use **PaddleOCR** for everything - it's simpler and has built-in MCP support

**Architecture**:
```
User uploads any document
    ‚Üì
PaddleOCR (auto-detects layout, language, structure)
    ‚Üì
Return structured JSON
```

**Vercel Implementation**:
```javascript
// api/ocr-paddle.js
export const config = { runtime: 'edge' };

export default async function handler(req) {
  const { image, pipeline } = await req.json();
  
  // pipeline can be: 'ocr', 'structure', 'chatocr'
  const response = await fetch(`${process.env.PADDLE_OCR_URL}/${pipeline}`, {
    method: 'POST',
    headers: { 'Content-Type': 'application/json' },
    body: JSON.stringify({
      file: image,
      fileType: 1
    })
  });
  
  const result = await response.json();
  return new Response(JSON.stringify(result));
}
```

**Pros**:
- ‚úÖ Single OCR server to maintain
- ‚úÖ Built-in MCP support (could integrate with VS Code)
- ‚úÖ 100+ languages out of box
- ‚úÖ Apache 2.0 = commercial use allowed

**Cons**:
- ‚ùå Slightly lower accuracy than HunyuanOCR for invoices (80% vs 92%)

---

### **Option C: HunyuanOCR Only (BEST ACCURACY)**
Use **HunyuanOCR** for all OCR - highest accuracy

**Architecture**:
```
User uploads document
    ‚Üì
HunyuanOCR (1B param specialist)
    ‚Üì
Return structured JSON
```

**Pros**:
- ‚úÖ **92%+ accuracy** on invoices/receipts
- ‚úÖ SOTA table parsing and formula extraction
- ‚úÖ Lightweight (1B params)

**Cons**:
- ‚ùå Fewer languages than PaddleOCR (mostly Chinese/English)
- ‚ùå Requires vLLM deployment

---

## Deployment Strategy

### **Self-Hosted (FREE but requires server)**

**Step 1: Deploy HunyuanOCR**
```bash
# On your server (e.g., AWS EC2, DigitalOcean, local)
uv venv hunyuanocr
source hunyuanocr/bin/activate
uv pip install -U vllm --pre --extra-index-url https://wheels.vllm.ai/nightly

# Start server
vllm serve tencent/HunyuanOCR \
    --host 0.0.0.0 \
    --port 8118 \
    --gpu-memory-utilization 0.2
```

**Step 2: Deploy PaddleOCR**
```bash
pip install paddleocr paddlex
paddleocr serve --pipeline ocr --host 0.0.0.0 --port 8080
```

**Step 3: Update Vercel env vars**
```bash
# In Vercel dashboard ‚Üí Settings ‚Üí Environment Variables
HUNYUAN_OCR_URL=http://your-server-ip:8118
PADDLE_OCR_URL=http://your-server-ip:8080
```

---

### **Cloud Hosting (EASIEST)**

**Option 1: Railway.app** (FREE tier: 500 hours/month)
1. Fork `Awehbelekker/HunyuanOCR` to your account
2. Connect Railway to your GitHub
3. Deploy ‚Üí auto-detects Python, installs deps
4. Copy service URL ‚Üí add to Vercel env

**Option 2: Hugging Face Spaces** (FREE GPU!)
1. Create new Space on HF
2. Upload HunyuanOCR code
3. Enable GPU (free tier: T4)
4. Space URL ‚Üí add to Vercel env

**Option 3: Modal** (FREE $30/month credit)
```python
# modal_hunyuan.py
import modal

app = modal.App("hunyuan-ocr")

@app.function(gpu="T4", image=modal.Image.debian_slim().pip_install("vllm"))
def ocr_inference(image_base64: str):
    from vllm import LLM
    llm = LLM(model="tencent/HunyuanOCR")
    # ... inference code
    return result

@app.function()
@modal.web_endpoint()
def api(image: str):
    return ocr_inference.remote(image)
```

---

## Integration Code for Your System

### **Enhanced OCR Endpoints**

Replace your current `api/ocr-invoice.js`, `api/ocr-pricelist.js`, `api/ocr-customer.js` with production OCR:

```javascript
// api/ocr-hunyuan-invoice.js
export const config = { runtime: 'edge' };

export default async function handler(req) {
  const { image } = await req.json();
  
  try {
    // Call HunyuanOCR server
    const response = await fetch(process.env.HUNYUAN_OCR_URL + '/v1/chat/completions', {
      method: 'POST',
      headers: { 'Content-Type': 'application/json' },
      body: JSON.stringify({
        model: 'tencent/HunyuanOCR',
        messages: [{
          role: 'user',
          content: [
            { type: 'image_url', image_url: { url: `data:image/jpeg;base64,${image}` }},
            { type: 'text', text: `Extract invoice data as JSON with fields: 
              {
                "supplier": "company name",
                "invoiceNumber": "inv-123",
                "date": "YYYY-MM-DD",
                "items": [{"description": "", "qty": 0, "unitPrice": 0, "total": 0}],
                "subtotal": 0,
                "vat": 0,
                "total": 0,
                "dueDate": "YYYY-MM-DD"
              }` }
          ]
        }],
        temperature: 0.1,
        max_tokens: 2000
      })
    });
    
    const result = await response.json();
    const content = result.choices[0].message.content;
    
    // Extract JSON from response
    const jsonMatch = content.match(/```json\n([\s\S]*?)\n```/) || content.match(/\{[\s\S]*\}/);
    const invoiceData = JSON.parse(jsonMatch[1] || jsonMatch[0]);
    
    return new Response(JSON.stringify({
      success: true,
      data: invoiceData,
      accuracy: 'high', // HunyuanOCR = 92%+ on invoices
      source: 'hunyuan-ocr'
    }));
    
  } catch (error) {
    // Fallback to current Llama Vision if HunyuanOCR unavailable
    const fallbackUrl = 'https://api.together.xyz/v1/chat/completions';
    const response = await fetch(fallbackUrl, {
      method: 'POST',
      headers: {
        'Content-Type': 'application/json',
        'Authorization': 'Bearer ' + process.env.TOGETHER_API_KEY
      },
      body: JSON.stringify({
        model: 'meta-llama/Llama-3.2-11B-Vision-Instruct-Turbo',
        messages: [/* same as above */]
      })
    });
    
    return new Response(JSON.stringify({
      success: true,
      data: await response.json(),
      accuracy: 'medium',
      source: 'llama-vision-fallback'
    }));
  }
}
```

### **Frontend Integration**

Update your scan functions to use the new OCR:

```javascript
// COMPLETE-INVOICE-SYSTEM.html (add to existing code)

async function scanSupplierInvoiceEnhanced(imageData) {
  showToast('üîç Scanning with production OCR...', 'info');
  
  try {
    // Try HunyuanOCR first (92% accuracy)
    const response = await fetch('/api/ocr-hunyuan-invoice', {
      method: 'POST',
      headers: { 'Content-Type': 'application/json' },
      body: JSON.stringify({ image: imageData })
    });
    
    const result = await response.json();
    
    if (result.success) {
      showToast(`‚úÖ Invoice scanned! (${result.source}, ${result.accuracy} accuracy)`, 'success');
      return result.data;
    }
  } catch (error) {
    console.error('Enhanced OCR failed, using fallback:', error);
    // Falls back to Llama Vision automatically
  }
}

async function scanPriceListEnhanced(imageData, supplierName) {
  showToast('üîç Scanning price list with multilingual OCR...', 'info');
  
  try {
    // Use PaddleOCR for multilingual support
    const response = await fetch('/api/ocr-paddle-pricelist', {
      method: 'POST',
      headers: { 'Content-Type': 'application/json' },
      body: JSON.stringify({ 
        image: imageData,
        supplier: supplierName,
        detectLanguage: true // Auto-detect 100+ languages
      })
    });
    
    const result = await response.json();
    
    if (result.success) {
      showToast(`‚úÖ Scanned ${result.count} products! (Lang: ${result.language})`, 'success');
      return result.products;
    }
  } catch (error) {
    console.error('PaddleOCR failed:', error);
  }
}
```

---

## Comparison Matrix

| Feature | Current (Llama Vision) | HunyuanOCR | PaddleOCR |
|---------|----------------------|------------|-----------|
| **Invoice Accuracy** | ~60-70% | **92%+** | ~80% |
| **Languages** | English, basic | Chinese, English | **100+** |
| **Table Parsing** | Text only | **HTML format** | HTML + structure |
| **Formula Support** | No | **LaTeX format** | Limited |
| **License** | Free (Together AI) | Open Source | **Apache 2.0** |
| **Hosting Cost** | FREE | FREE (self-host) | FREE (self-host) |
| **Setup Complexity** | None (API) | Medium (vLLM) | Low (pip install) |
| **MCP Support** | No | No | **Yes (built-in)** |

---

## Recommended Implementation

### **Phase 1: Add PaddleOCR (Week 1)**
- ‚úÖ Easiest to deploy (`pip install paddleocr`)
- ‚úÖ Built-in serving
- ‚úÖ 100+ languages immediately
- ‚úÖ MCP server included

### **Phase 2: Add HunyuanOCR (Week 2)**
- ‚úÖ Higher accuracy for invoices/receipts
- ‚úÖ Better table + formula extraction
- ‚úÖ Use as primary, PaddleOCR as fallback

### **Phase 3: Optimize (Week 3)**
- ‚úÖ Auto-route based on document type
- ‚úÖ Language detection ‚Üí PaddleOCR
- ‚úÖ Invoice/receipt ‚Üí HunyuanOCR
- ‚úÖ Add confidence scoring

---

## Cost Analysis

### **Current System**:
- Together AI: **FREE** (but limited accuracy)

### **Hybrid System**:
- **Self-hosted**: FREE (if you have server)
  - AWS EC2 t2.medium: ~$30/month
  - DigitalOcean droplet: ~$12/month
  - Home server: FREE (electricity only)

- **Cloud-hosted**:
  - Railway.app: **FREE** (500 hrs/month)
  - Hugging Face Spaces: **FREE** (with GPU!)
  - Modal: **FREE** ($30 credit/month)

**Recommendation**: Start with **Hugging Face Spaces** (FREE GPU) for HunyuanOCR, local server for PaddleOCR.

---

## Next Steps

1. **Choose integration option** (A, B, or C)
2. **Deploy OCR servers**:
   - HunyuanOCR on HF Spaces (if Option A or C)
   - PaddleOCR locally or Railway (if Option A or B)
3. **Create new API endpoints** (`/api/ocr-hunyuan-*`, `/api/ocr-paddle-*`)
4. **Update frontend** to use enhanced OCR
5. **Add language detection** for PaddleOCR routing
6. **Test with real invoices** and compare accuracy
7. **Deploy to Vercel** with new env vars

---

## Questions to Answer

1. **Do you have a server** to self-host OCR? (If no ‚Üí use HF Spaces)
2. **Primary use case**: 
   - Invoices/receipts only ‚Üí **HunyuanOCR** (Option C)
   - Multilingual documents ‚Üí **PaddleOCR** (Option B)
   - Both ‚Üí **Hybrid** (Option A)
3. **Budget**: 
   - $0 ‚Üí Self-host or HF Spaces
   - $10-30/month ‚Üí Railway/Modal
4. **Technical skill**: 
   - Basic ‚Üí PaddleOCR only (easiest)
   - Advanced ‚Üí Hybrid with auto-routing

Let me know which option you prefer and I'll generate the complete implementation code!
