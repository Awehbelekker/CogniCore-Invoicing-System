# ============================================================================
# CogniCore Invoice System - Docker Compose
# One-click Hybrid Deployment (Local AI + Cloud Fallback)
# Version 2.0 - With GLM Support & Auto-Detection
# ============================================================================
# Usage:
#   docker-compose up -d                    # Start all services
#   docker-compose down                     # Stop all services
#   docker-compose logs -f                  # View logs
#   docker-compose --profile glm up -d      # Start with GLM models
#   docker-compose --profile enterprise up  # Start enterprise profile
# ============================================================================

version: '3.8'

# =========================================
# Environment Variable Defaults
# =========================================
x-common-env: &common-env
  NODE_ENV: production
  OLLAMA_URL: http://ollama:11434
  # Cloud API Keys (optional - set in .env file)
  TOGETHER_API_KEY: ${TOGETHER_API_KEY:-}
  OPENROUTER_API_KEY: ${OPENROUTER_API_KEY:-}
  ZAI_API_KEY: ${ZAI_API_KEY:-}
  OPENAI_API_KEY: ${OPENAI_API_KEY:-}
  ANTHROPIC_API_KEY: ${ANTHROPIC_API_KEY:-}
  GOOGLE_AI_KEY: ${GOOGLE_AI_KEY:-}
  NOVITA_API_KEY: ${NOVITA_API_KEY:-}
  SILICONFLOW_API_KEY: ${SILICONFLOW_API_KEY:-}

services:
  # =========================================
  # Web Application (Nginx)
  # =========================================
  cognicore-web:
    image: nginx:alpine
    container_name: cognicore-web
    ports:
      - "80:80"
      - "443:443"
    volumes:
      - ../:/usr/share/nginx/html:ro
      - ./nginx.conf:/etc/nginx/conf.d/default.conf:ro
    depends_on:
      - cognicore-api
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "wget", "-q", "--spider", "http://localhost/"]
      interval: 30s
      timeout: 10s
      retries: 3

  # =========================================
  # API Server (Node.js Express)
  # =========================================
  cognicore-api:
    build:
      context: .
      dockerfile: Dockerfile.api
    container_name: cognicore-api
    ports:
      - "3001:3001"
    environment:
      <<: *common-env
      MODEL: ${MODEL:-glm4}
      FALLBACK_MODEL: ${FALLBACK_MODEL:-llama3.1:8b}
    depends_on:
      ollama:
        condition: service_healthy
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "wget", "-q", "--spider", "http://localhost:3001/api/health"]
      interval: 30s
      timeout: 10s
      retries: 3

  # =========================================
  # Ollama (Local LLM Server) - CPU Only
  # =========================================
  ollama:
    image: ollama/ollama:latest
    container_name: cognicore-ollama
    ports:
      - "11434:11434"
    volumes:
      - ollama_data:/root/.ollama
    environment:
      - OLLAMA_HOST=0.0.0.0
      - OLLAMA_KEEP_ALIVE=24h
      - OLLAMA_NUM_PARALLEL=${OLLAMA_NUM_PARALLEL:-2}
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "ollama", "list"]
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 60s

  # =========================================
  # Ollama with GPU (NVIDIA) - Use this for GPU support
  # =========================================
  ollama-gpu:
    image: ollama/ollama:latest
    container_name: cognicore-ollama-gpu
    ports:
      - "11434:11434"
    volumes:
      - ollama_data:/root/.ollama
    environment:
      - OLLAMA_HOST=0.0.0.0
      - OLLAMA_KEEP_ALIVE=24h
      - OLLAMA_NUM_PARALLEL=${OLLAMA_NUM_PARALLEL:-4}
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu]
    restart: unless-stopped
    profiles:
      - gpu
    healthcheck:
      test: ["CMD", "ollama", "list"]
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 60s

  # =========================================
  # Smart Model Loader - Auto-detects and downloads best models
  # =========================================
  model-loader:
    image: ollama/ollama:latest
    container_name: cognicore-model-loader
    depends_on:
      ollama:
        condition: service_healthy
    environment:
      - OLLAMA_HOST=ollama:11434
      - PRIMARY_MODEL=${MODEL:-glm4}
      - SECONDARY_MODEL=${FALLBACK_MODEL:-llama3.1:8b}
    entrypoint: ["/bin/sh", "-c"]
    command:
      - |
        echo "â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—"
        echo "â•‘     CogniCore AI Model Loader - v2.0 with GLM Support    â•‘"
        echo "â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•"
        echo ""
        echo "ðŸ”„ Installing primary model: $PRIMARY_MODEL"
        ollama pull $PRIMARY_MODEL || echo "âš ï¸ Primary model failed"
        echo ""
        echo "ðŸ”„ Installing fallback model: $SECONDARY_MODEL"
        ollama pull $SECONDARY_MODEL || echo "âš ï¸ Fallback model failed"
        echo ""
        echo "âœ… Model installation complete!"
        echo ""
        echo "ðŸ“‹ Available models:"
        ollama list
    restart: "no"

  # =========================================
  # GLM Model Loader Profile (GLM-4-9B + GLM-Z1-9B)
  # =========================================
  glm-model-loader:
    image: ollama/ollama:latest
    container_name: cognicore-glm-loader
    depends_on:
      ollama:
        condition: service_healthy
    environment:
      - OLLAMA_HOST=ollama:11434
    profiles:
      - glm
    entrypoint: ["/bin/sh", "-c"]
    command:
      - |
        echo "ðŸš€ Installing GLM models (Recommended for CogniCore)..."
        echo ""
        echo "ðŸ“¦ GLM-4-9B (General purpose, multilingual, long-context)"
        ollama pull glm4 || true
        echo ""
        echo "ðŸ“¦ GLM-Z1-9B (Reasoning, math, logic)"
        ollama pull hf.co/bartowski/GLM-Z1-9B-0414-GGUF:Q4_K_M || true
        echo ""
        echo "ðŸ“¦ Llama 3.2 3B (Lightweight fallback)"
        ollama pull llama3.2:3b || true
        echo ""
        echo "âœ… GLM models installed!"
        ollama list
    restart: "no"

  # =========================================
  # Enterprise Model Loader (32B models)
  # =========================================
  enterprise-model-loader:
    image: ollama/ollama:latest
    container_name: cognicore-enterprise-loader
    depends_on:
      ollama:
        condition: service_healthy
    environment:
      - OLLAMA_HOST=ollama:11434
    profiles:
      - enterprise
    entrypoint: ["/bin/sh", "-c"]
    command:
      - |
        echo "ðŸ¢ Installing Enterprise models (32B - requires 24GB+ VRAM)..."
        echo ""
        echo "ðŸ“¦ GLM-4-32B (Best quality, coding, agents)"
        ollama pull hf.co/bartowski/GLM-4-32B-0414-GGUF:Q4_K_M || true
        echo ""
        echo "ðŸ“¦ GLM-Z1-32B (Deep reasoning)"
        ollama pull hf.co/bartowski/GLM-Z1-32B-0414-GGUF:Q4_K_M || true
        echo ""
        echo "ðŸ“¦ Llama 3.1 70B (Backup)"
        ollama pull llama3.1:70b || true
        echo ""
        echo "âœ… Enterprise models installed!"
        ollama list
    restart: "no"

  # =========================================
  # Lightweight Model Loader (for low-end hardware)
  # =========================================
  lite-model-loader:
    image: ollama/ollama:latest
    container_name: cognicore-lite-loader
    depends_on:
      ollama:
        condition: service_healthy
    environment:
      - OLLAMA_HOST=ollama:11434
    profiles:
      - lite
    entrypoint: ["/bin/sh", "-c"]
    command:
      - |
        echo "ðŸ’¡ Installing lightweight models (for low VRAM/RAM)..."
        echo ""
        echo "ðŸ“¦ Llama 3.2 1B (Ultra-light, ~1GB RAM)"
        ollama pull llama3.2:1b || true
        echo ""
        echo "ðŸ“¦ Qwen 2.5 0.5B (Tiny, <1GB RAM)"
        ollama pull qwen2.5:0.5b || true
        echo ""
        echo "ðŸ“¦ Phi-3 Mini (Small but smart)"
        ollama pull phi3:mini || true
        echo ""
        echo "âœ… Lightweight models installed!"
        ollama list
    restart: "no"

  # =========================================
  # Whisper (Optional - Local Voice Transcription)
  # =========================================
  whisper:
    image: onerahmet/openai-whisper-asr-webservice:latest
    container_name: cognicore-whisper
    ports:
      - "9000:9000"
    environment:
      - ASR_MODEL=${WHISPER_MODEL:-base}
    profiles:
      - voice
    restart: unless-stopped

# =========================================
# Volumes
# =========================================
volumes:
  ollama_data:
    name: cognicore_ollama_models

# =========================================
# Networks
# =========================================
networks:
  default:
    name: cognicore-network

