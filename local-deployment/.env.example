# ============================================================================
# CogniCore Environment Variables
# Copy this file to .env and fill in your API keys
# ============================================================================

# ===== Cloud AI Providers (Optional - for hybrid mode) =====

# Together AI - FREE tier (Recommended for cloud fallback)
TOGETHER_API_KEY=your_together_api_key_here

# Google Gemini - FREE tier available
GOOGLE_AI_KEY=your_google_ai_key_here

# OpenAI - Paid (if client has existing account)
OPENAI_API_KEY=your_openai_key_here

# Anthropic Claude - Paid (if client has existing account)
ANTHROPIC_API_KEY=your_anthropic_key_here

# ===== Local AI Settings =====

# Ollama URL (default: http://localhost:11434 or http://ollama:11434 in Docker)
OLLAMA_URL=http://ollama:11434

# Default model for local AI
MODEL=llama3.1:8b

# ===== Application Settings =====

# Node environment
NODE_ENV=production

# API server port
PORT=3001

# ===== Master Admin Settings =====

# Master admin can override client settings
MASTER_ADMIN_KEY=your_secret_master_key

# Allow master admin to push API keys to clients
ALLOW_REMOTE_CONFIG=true

